{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 5 - AI Lab\n",
        "\n",
        "Author: Khushee Kapoor\n",
        "\n",
        "Registration Number: 200968052\n"
      ],
      "metadata": {
        "id": "Try-9Vrvo6Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "bKwplNqopANj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To set up, we first install tf-agents using pip"
      ],
      "metadata": {
        "id": "Wq8JZCGhpBYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing tf-agents\n",
        "!pip install tf-agents"
      ],
      "metadata": {
        "id": "sgexfziKWWQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import the following libraries:\n",
        "- abc: to create abstract classes\n",
        "- numpy: for computations\n",
        "- tensorflow: to create environments"
      ],
      "metadata": {
        "id": "iD-lb0BepFm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mU9uFQS1VzZ0"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import abc\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we import the packagaes, classes, and functions required to create the environments"
      ],
      "metadata": {
        "id": "r52hqmEQpPOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages, classes, and functions to create the environments\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "nest = tf.nest"
      ],
      "metadata": {
        "id": "HZSw242yWSoz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the custom environments, first we create the BanditPyEnvironment which the custom environments will inherit.\n",
        "\n",
        "The BanditPyEnvironment class inherits from the py_environment.PyEnvironment class, which is a base class provided by TF-Agents for implementing custom environments. The __ init __ method initializes the observation and action specifications of the environment.\n",
        "\n",
        "The action_spec and observation_spec methods return the respective specifications for the environment. The _empty_observation method returns an empty observation with the same shape and data type as the observation specification.\n",
        "\n",
        "The _reset and _step methods are defined by the py_environment.PyEnvironment class and should not be overridden by subclasses. _reset returns a time step containing an observation, and _step returns a time step containing the reward for the action taken. The _observe and _apply_action methods are abstract methods that must be implemented in subclasses.\n",
        "\n",
        "The _observe method returns the current observation of the environment. The _apply_action method applies the given action to the environment and returns the corresponding reward.\n",
        "\n",
        "Overall, this class provides a framework for defining a custom reinforcement learning environment that can be used with TF-Agents. Subclasses of this class can implement their own _observe and _apply_action methods to define the specific behavior of the environment."
      ],
      "metadata": {
        "id": "6zHJzAXWbzGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the MultiArmedBandit Environment\n",
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  # init function to initialize the observation and action specifications\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  # intialize the action specification\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  # initialize the observation specification\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  # returns an empty observation\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  # returns a time step containing an observation\n",
        "  def _reset(self):\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  # returns a time step containing the reward for the action taken\n",
        "  def _step(self, action):\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # abstract method returns an observation\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    pass\n",
        "\n",
        "  # applies action to the Environment and returns the corresponding reward\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    pass"
      ],
      "metadata": {
        "id": "xoelcVLUWNMO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1"
      ],
      "metadata": {
        "id": "V58Tvy0CeIW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the custom environment for Exercise 1, we create a subclass of the BanditPyEnvironment class defined previously. The Environment1 class represents a simple reinforcement learning environment in which the agent observes a random integer between -5 and 5 and chooses an action from the set {0, 1, 2}. The reward for each action is the product of the observation and the action value.\n",
        "\n",
        "The __ init __ method initializes the observation and action specifications for this environment using the BoundedArraySpec class from TF-Agents. The observation specification is a 1D array with a single integer element between -5 and 5, while the action specification is a scalar integer between 0 and 2.\n",
        "\n",
        "The _observe method generates a random observation within the range [-5, 5] and returns it as a 1D array. The _apply_action method takes an action as input, multiplies it by the current observation, and returns the result as the reward for the action.\n",
        "\n",
        "Overall, this class provides a simple environment for testing reinforcement learning algorithms that operate on discrete actions and integer observations. Subclasses of this class can modify the observation and action specifications and implement their own _observe and _apply_action methods to define custom environments for their specific use case."
      ],
      "metadata": {
        "id": "zAxnVaPfckmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a custom environment\n",
        "class Environment1(BanditPyEnvironment):\n",
        "\n",
        "  # intializing the actions and observations specifications\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(Environment1, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  # generates a random observation\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5, 6, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  # returns the reward = action * observation\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ],
      "metadata": {
        "id": "YvAouhaJV-iM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a TensorFlow policy called SignPolicy, which is used to map observations to actions in an RL environment.\n",
        "\n",
        "The __ init __ method initializes the observation and action specifications in the same way as the previous examples.\n",
        "\n",
        "The _distribution method is not implemented in this code snippet. This method is used to define the probability distribution of actions given the current state of the environment. In this case, since the policy is deterministic, the method is not implemented.\n",
        "\n",
        "The _variables method is also not implemented in this code snippet. This method is used to return the variables of the policy, which can be used to save or restore the policy. Since this policy does not have any variables, the method returns an empty tuple.\n",
        "\n",
        "The _action method takes as input a time step, which contains the current observation of the environment, a policy state, and a seed. It computes the sign of the observation using tf.sign and casts it to an integer using tf.cast. It then adds 1 to the sign to obtain the action. Specifically, if the observation is negative, the action is 0, if it is positive, the action is 2, and if it is zero, the action is 1. Finally, it returns a PolicyStep object containing the action and the policy state.\n",
        "\n",
        "Overall, this policy defines a simple mapping between observations and actions, which is based on the sign of the observation. The policy is deterministic, meaning that it always maps the same observation to the same action. The policy can be used to control the behavior of an agent in an RL environment."
      ],
      "metadata": {
        "id": "vl3fAPluc4bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an optimal policy\n",
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "\n",
        "  # initializing the observation, timestep, and action specifications\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "    \n",
        "  # return the probability distribution\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  # to return variables of the policy\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  # returns action and policy state\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "aJkciTvlXLY8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An instance of Environment1 is created and wrapped as a TensorFlow environment using tf_py_environment.TFPyEnvironment(). An instance of SignPolicy is also created.\n",
        "\n",
        "Then, a for loop runs 50 iterations. In each iteration, the environment is reset using tf_environment.reset(), which returns a time_step object that contains the current observation. This time_step object is passed to the sign_policy object using sign_policy.action(current_time_step).action, which returns an action for the current observation. The action is then passed to the environment using tf_environment.step(action), which returns a time_step object that contains the new observation and the reward obtained from taking the action. The loop calculates the total reward by summing up the reward earned in each iteration."
      ],
      "metadata": {
        "id": "hQpgCaQQeESU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating instance of environment\n",
        "environment = Environment1()\n",
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)\n",
        "\n",
        "# creating instance of policy\n",
        "sign_policy = SignPolicy()\n",
        "\n",
        "# calculating total reward for 50 observations\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "  current_time_step = tf_environment.reset()\n",
        "  action = sign_policy.action(current_time_step).action\n",
        "  reward = tf_environment.step(action).reward\n",
        "  total_reward += reward\n",
        "\n",
        "print('Total Reward: ', total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipdwRsCoXkgK",
        "outputId": "97979360-8483-47dd-bc0f-3e3ec637aa37"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward:  tf.Tensor([[136.]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the total reward is 136."
      ],
      "metadata": {
        "id": "bCfSuHwUqHCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2"
      ],
      "metadata": {
        "id": "T__FiMLSeLGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a custom environment for Exercise 2, we define a custom environment class called Environment2, which is a subclass of BanditPyEnvironment. It defines an environment where the reward for taking an action depends on the action and the current observation.\n",
        "\n",
        "The __ init __ method initializes the environment by defining the action_spec and observation_spec. The observation_spec is a bounded array of shape (1,) with integer dtype and minimum value of -5 and maximum value of 5. The action_spec is a bounded array of shape () with integer dtype and minimum value of 0 and maximum value of 2.\n",
        "\n",
        "The _observe method generates a random observation within the range of -5 to 5 (inclusive) and returns it.\n",
        "\n",
        "The _apply_action method applies the action taken by the agent to the environment and returns the corresponding reward. The reward is computed by multiplying the observation with the action and a random reward sign generated during initialization. The reward sign is set to be either -1 or 1 with equal probability.\n",
        "\n",
        "Finally, a Environment2 instance is wrapped in a TensorFlow environment using tf_py_environment.TFPyEnvironment and assigned to the variable two_way_tf_environment."
      ],
      "metadata": {
        "id": "Ok7SOj1geZlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a custom environment\n",
        "class Environment2(BanditPyEnvironment):\n",
        "\n",
        "  # intializing the actions, observations specifications, and reward\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(Environment2, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  # generating a random observation\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5, 6, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  # returns reward = action * observation * reward sign\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n",
        "\n",
        "# creating instance of environment\n",
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(Environment2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chRq0CLCXn8m",
        "outputId": "c824084e-40b7-4a53-d9ae-3b3d36248a67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward sign:\n",
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the environment has been assigned the negative sign by sheer probability."
      ],
      "metadata": {
        "id": "qbfIFKwjtw6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a custom policy class called TwoWaySignPolicy that extends tf_policy.TFPolicy. This policy class takes in a situation parameter, which represents the current situation in the environment. The __ init __ method sets up the observation and action specs, as well as the time step spec. It also sets the situation attribute.\n",
        "\n",
        "The _distribution and _variables methods do not do anything in this implementation, and simply return None and the situation attribute, respectively.\n",
        "\n",
        "The _action method is the most important method in this policy. It takes in the current time step, policy state, and random seed, and returns an action. The action is determined based on the situation attribute and the sign of the observation. If the situation is 0, the policy chooses action 1 (unknown). If the situation is 1 and the observation is positive, the policy chooses action 2 (positive). If the situation is 1 and the observation is negative, the policy chooses action 0 (negative). If the situation is 2, the policy chooses the opposite action to what it would choose in situation 1.\n",
        "\n",
        "Overall, this policy is designed to handle a two-way bandit environment where the reward sign can be flipped. It takes into account the current situation and the sign of the observation to make decisions about which action to take."
      ],
      "metadata": {
        "id": "rP2pg7ZAeu1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a custom policy\n",
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "\n",
        "  # initializing the observation, action, timestep specifications\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  # returns the probability distribution\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  # returns the variables\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  # determines and returns the action based on the sign\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      return tf.constant(1, shape=(1,))\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "Nifq3bKhY_k-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a reinforcement learning agent called SignAgent which inherits from the TFAgent class. The agent uses a policy called TwoWaySignPolicy to choose actions based on observations from the environment. The TwoWaySignPolicy class is initialized with a situation variable that is used to determine the behavior of the policy.\n",
        "\n",
        "The SignAgent constructor initializes the TwoWaySignPolicy policy and sets the time_step_spec and action_spec to match the policy's specs. It also sets the policy, collect_policy, and train_sequence_length attributes by calling the parent class constructor.\n",
        "\n",
        "The _initialize method initializes the variables of the SignAgent.\n",
        "\n",
        "The _train method updates the SignAgent by training it on a batch of experience. The method checks if the current situation requires action and updates the situation accordingly using a conditional statement. Finally, the method returns a LossInfo object with empty tensors for the loss and the regularization loss."
      ],
      "metadata": {
        "id": "0ne48VkhfQ9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an agent\n",
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  # intializing the variables of the agent\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  # updating the agent by training it\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "    def new_situation_fn():\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()"
      ],
      "metadata": {
        "id": "nolcupPIZKy8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function trajectory_for_bandit creates a Trajectory object for a bandit problem given an initial time step, an action time step, and a final time step.\n",
        "\n",
        "The Trajectory object is a container for the different elements that make up a sequence of observations, actions, rewards, and other information during an interaction with an environment. In this case, the Trajectory object is created with the following arguments:\n",
        "\n",
        "- observation: A tensor of shape (1, 1) representing the initial observation.\n",
        "- action: A tensor of shape (1,) representing the action taken at the action step.\n",
        "- policy_info: A dictionary containing any additional information about the action taken, if available.\n",
        "- reward: A tensor of shape (1,) representing the reward received at the final step.\n",
        "- discount: A tensor of shape (1,) representing the discount factor at the final step.\n",
        "- step_type: A tensor of shape (1,) representing the type of the initial step (i.e., whether it is a FIRST or MID step).\n",
        "- next_step_type: A tensor of shape (1,) representing the type of the final step (i.e., whether it is a MID or LAST step).\n",
        "\n",
        "By creating a Trajectory object, the function creates a container that can be used to store and manipulate the data associated with a single interaction with the environment. This can be useful for constructing datasets for training reinforcement learning agents."
      ],
      "metadata": {
        "id": "EurjMNwtfu9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a trajectory for the bandit\n",
        "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
        "                               action=tf.expand_dims(action_step.action, 0),\n",
        "                               policy_info=action_step.info,\n",
        "                               reward=tf.expand_dims(final_step.reward, 0),\n",
        "                               discount=tf.expand_dims(final_step.discount, 0),\n",
        "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
        "                               next_step_type=tf.expand_dims(final_step.step_type, 0))"
      ],
      "metadata": {
        "id": "oCFU61BnZWvj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test out the enivronment, we simulate the interaction between the SignAgent and the Environment2 for 10 steps.\n",
        "\n",
        "- The initial step is obtained by resetting the environment, and stored in the variable step.\n",
        "- The agent's collect_policy is used to select an action based on the current step.\n",
        "- The action is taken in the environment using the step function, and the resulting next_step is obtained.\n",
        "- A Trajectory object is created using the current step, the selected action, and the resulting next_step.\n",
        "- The agent's train method is called with the Trajectory object as input.\n",
        "- The next_step becomes the new step, and the process repeats from step 2 for the next iteration.\n",
        "\n",
        "The purpose of this code is to train the SignAgent to learn a policy for selecting actions in the Environment2. The experience generated from each interaction is used to update the agent's policy."
      ],
      "metadata": {
        "id": "HqpyhIwAgEfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simulating the trajectory of the agent in the environment\n",
        "step = two_way_tf_environment.reset()\n",
        "for _ in range(10):\n",
        "  action_step = sign_agent.collect_policy.action(step)\n",
        "  next_step = two_way_tf_environment.step(action_step.action)\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
        "  print(experience)\n",
        "  sign_agent.train(experience)\n",
        "  step = next_step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYeA0BezZkGE",
        "outputId": "c60b7995-e13f-4d68-9da9-3c90846d60de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-5]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[5.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-4]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[8.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[4]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n",
            "Trajectory(\n",
            "{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n",
            " 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n",
            " 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-4]]], dtype=int32)>,\n",
            " 'policy_info': (),\n",
            " 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[8.]], dtype=float32)>,\n",
            " 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n"
          ]
        }
      ]
    }
  ]
}