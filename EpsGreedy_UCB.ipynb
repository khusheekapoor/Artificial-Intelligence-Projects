{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 6 - AI Lab\n",
        "\n",
        "Author: Khushee Kapoor\n",
        "\n",
        "Registration Number: 200968052"
      ],
      "metadata": {
        "id": "_BDMelNWTaDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem agent formulation involves determining the most optimal ad to display to a user at a given time instant to maximize the number of clicks.\n",
        "\n",
        "- State: The state of the system can be defined as the ad's historical data of CTR (click through rate).\n",
        "\n",
        "- Action: The action space consists of the ads that can be displayed as per the exploration or exploitation approach.\n",
        "\n",
        "- Reward: The reward function can be designed to maximize the CTR. If for a particular timestep, the user has clicked on the ad, then the reward is 1 otherwise 0.\n",
        "\n",
        "- Environment: There are 10 different ads to choose from and the goal is to maximize the total number of clicks received by the company. At each time step, the MAB agent must select one of the ads to display to the user. After the ad is displayed, the agent observes whether or not the user clicks on the ad.\n",
        "\n",
        "- Policy: The MAB agent has to learn a policy that maps the current state of the environment (i.e., which ads have been shown and clicked on in the past) to a decision (i.e., which ad to display next). The policy should take into account the uncertainty in the click probabilities of the ads and the expected reward of choosing each ad. The goal of the MAB agent is to learn the optimal policy that maximizes the total number of clicks received from the users over a given period of time.\n",
        "\n",
        "The MAB agent's objective is to learn the true CTR of each ad while minimizing the regret, which is the difference between the expected number of clicks obtained by displaying the best ad and the expected number of clicks obtained by displaying the chosen ad at each time step. The MAB agent must balance the exploration of less-known ads to learn their CTRs with the exploitation of the ads that are known to have higher CTRs to maximize the total number of clicks."
      ],
      "metadata": {
        "id": "WhsPvhZgblRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we first import the following libraries:\n",
        "\n",
        "- NumPy: for mathematical computations\n",
        "- Pandas: for data manipulation"
      ],
      "metadata": {
        "id": "XkQ7VFdgTimK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvxDXannyB9P"
      },
      "outputs": [],
      "source": [
        "# importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import the Ads Optimization Data using the read_csv() function from the Pandas library."
      ],
      "metadata": {
        "id": "jwc6PB0HTrXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "df = pd.read_csv('Ads_Optimisation.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5ysTtAIVzhSr",
        "outputId": "9a342d08-cdfa-4feb-ab0b-1e2d63821eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
              "0     1     0     0     0     1     0     0     0     1      0\n",
              "1     0     0     0     0     0     0     0     0     1      0\n",
              "2     0     0     0     0     0     0     0     0     0      0\n",
              "3     0     1     0     0     0     0     0     1     0      0\n",
              "4     0     0     0     0     0     0     0     0     0      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fed5a2a7-2f59-4996-bd1a-0d3c44ca4734\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ad 1</th>\n",
              "      <th>Ad 2</th>\n",
              "      <th>Ad 3</th>\n",
              "      <th>Ad 4</th>\n",
              "      <th>Ad 5</th>\n",
              "      <th>Ad 6</th>\n",
              "      <th>Ad 7</th>\n",
              "      <th>Ad 8</th>\n",
              "      <th>Ad 9</th>\n",
              "      <th>Ad 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fed5a2a7-2f59-4996-bd1a-0d3c44ca4734')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fed5a2a7-2f59-4996-bd1a-0d3c44ca4734 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fed5a2a7-2f59-4996-bd1a-0d3c44ca4734');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epsilon Greedy"
      ],
      "metadata": {
        "id": "mO-GIgqhz20Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the epsilon greedy approach, we first define the reward function. In this function, we see what is the reward for a particular action at a particular timesteps and return it."
      ],
      "metadata": {
        "id": "bDgIMbVAUJBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the reward function for epislon greedy\n",
        "def reward(t, A):\n",
        "  ad = 'Ad {}'.format(A+1)\n",
        "  reward = df.loc[df.index==t, ad]\n",
        "  return reward.values[0]"
      ],
      "metadata": {
        "id": "MVrY5sPW8nV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the epsilon greedy algorithm with eps = 0.01. We use dictionaries to store the values of number of steps of different actions. "
      ],
      "metadata": {
        "id": "ZQsMCuGyVIcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the parameters\n",
        "timesteps = 2000\n",
        "k = 10\n",
        "e = 0.01\n",
        "\n",
        "# initializing the probabilities\n",
        "probs = np.random.random(size=timesteps)\n",
        "\n",
        "# initializing total rewards\n",
        "tot_R = 0\n",
        "\n",
        "# dictionaries to store the values and number of steps\n",
        "Q = {}\n",
        "N = {}\n",
        "\n",
        "# initiializing values and number of stepss to be 0\n",
        "for i in range(k):\n",
        "  Q[i] = 0\n",
        "  N[i] = 0\n",
        "\n",
        "# epsilon greedy algorithm\n",
        "for t in range(timesteps):\n",
        "  if probs[t]>e:\n",
        "    A = max(zip(Q.values(), Q.keys()))[1] # exploitation\n",
        "  else:\n",
        "    A = np.random.randint(k) # exploration\n",
        "\n",
        "  # updating values of parameters\n",
        "  R = reward(t, A)\n",
        "  N[A] = N[A] + 1\n",
        "  Q[A] = Q[A] + (R-Q[A])/N[A]\n",
        "  tot_R += R # computing total rewards\n",
        "\n",
        "print('Total Rewards: ', tot_R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irhbNNcczh8w",
        "outputId": "bdf5ad06-ad94-4f7f-d822-fa32f06d66f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rewards:  383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the epsilon greedy algorithm with eps = 0.3. We use dictionaries to store the values of number of steps of different actions. "
      ],
      "metadata": {
        "id": "BHVWxmwqXcHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the parameters\n",
        "timesteps = 2000\n",
        "k = 10\n",
        "e = 0.3\n",
        "\n",
        "# initializing the probabilities\n",
        "probs = np.random.random(size=timesteps)\n",
        "\n",
        "# initializing total rewards\n",
        "tot_R = 0\n",
        "\n",
        "# dictionaries to store the values and number of steps\n",
        "Q = {}\n",
        "N = {}\n",
        "\n",
        "# initiializing values and number of stepss to be 0\n",
        "for i in range(k):\n",
        "  Q[i] = 0\n",
        "  N[i] = 0\n",
        "\n",
        "# epsilon greedy algorithm\n",
        "for t in range(timesteps):\n",
        "  if probs[t]>e:\n",
        "    A = max(zip(Q.values(), Q.keys()))[1] # exploitation\n",
        "  else:\n",
        "    A = np.random.randint(0, 10) # exploration\n",
        "\n",
        "  # updating values of parameters\n",
        "  R = reward(t, A)\n",
        "  N[A] = N[A] + 1\n",
        "  Q[A] = Q[A] + (R-Q[A])/N[A]\n",
        "  tot_R += R # computing total rewards\n",
        "\n",
        "print('Total Rewards: ', tot_R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1kffSm42MqL",
        "outputId": "12400797-f748-4338-e795-2328b8840381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rewards:  436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the setting with a higher value of epsilon has a higher cummulative reward function value. This is because, higher the value of epsilon, higher is the exploration factor, hence, even though there are sub optimal payouts initially, in the long run, we have a higher probability of hitting the jackpot (in this case, picking an arm with higher reward)."
      ],
      "metadata": {
        "id": "q3UAy834X-D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ε-greedy approach estimates the value of an action using the sample average method, which calculates the average of rewards received for that action. However, this method can have high variance if the number of samples is low, resulting in slow convergence to true values. To address this, the ε-greedy approach explores more at the beginning to reduce uncertainty and exploits the best action based on estimated values later."
      ],
      "metadata": {
        "id": "COR255eFcBhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upper Confidence Bound"
      ],
      "metadata": {
        "id": "t1l5pQ4h6gVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the upper confidence bound approach, we first define the reward function. In this function, we see what is the reward for a particular action at a particular timesteps and return it."
      ],
      "metadata": {
        "id": "T0mO9LVTYnW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the reward function for upper confidence bound approach\n",
        "def reward(t, A):\n",
        "  ad = 'Ad {}'.format(A+1)\n",
        "  reward = df.loc[df.index==t, ad]\n",
        "  return reward.values[0]"
      ],
      "metadata": {
        "id": "ock3CMilDpfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement the upper confidence bound algorithm with c=1.5. "
      ],
      "metadata": {
        "id": "p1BAk04KYvhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the parameters of the algorithm\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "timesteps = 2000\n",
        "k = 10\n",
        "c = 1.5\n",
        "\n",
        "# initializing total rewards to 0\n",
        "tot_R = 0\n",
        "\n",
        "# initializing dictionaries\n",
        "N = {}\n",
        "R = {}\n",
        "\n",
        "for i in range(k):\n",
        "  N[i] = 1 # to avoid zero divide error\n",
        "  R[i] = 0\n",
        "\n",
        "# implementing the ucb algorithm\n",
        "for t in range(timesteps):\n",
        "  q = {}\n",
        "  delta = {}\n",
        "  ucb = {}\n",
        "\n",
        "  for i in range(k):\n",
        "    q[i] = R[i]/N[i] # average reward till timestep t\n",
        "    delta[i] = np.sqrt(c*np.log(t)/N[i]) # confidence interval\n",
        "    ucb[i] = q[i] + delta[i]\n",
        "  \n",
        "  # updating values of parameters\n",
        "  A = max(zip(ucb.values(), ucb.keys()))[1]\n",
        "  N[A] = N[A] + 1\n",
        "  R[A] = R[A] + reward(t, A)\n",
        "  tot_R += reward(t, A) # computing total reward\n",
        "\n",
        "print('Total Rewards: ', tot_R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8twRciu6fjr",
        "outputId": "85653963-1a24-4458-d9a6-2fb79a6ac1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rewards:  338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Upper-Confidence-Bound approach estimates action values using the Upper Confidence Bound (UCB), which combines the average reward and an uncertainty term. This term prioritizes actions that haven't been selected many times and gives lower priority to those that have. This approach results in more stable value estimates and faster convergence to the optimal action."
      ],
      "metadata": {
        "id": "B2BYVyiicF5J"
      }
    }
  ]
}