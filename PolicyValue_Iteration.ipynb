{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 8\n",
        "\n",
        "Author: Khushee Kapoor\n",
        "\n",
        "Registration Number: 200968052"
      ],
      "metadata": {
        "id": "ALSoy7ycGGLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get start, we import the required libraries and create the Frozen Lake Environment."
      ],
      "metadata": {
        "id": "5MwWp1iYGPz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBQWihK7c_4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff13637-c66f-4beb-f273-d71a4389540a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# importing the libraries and creating the environment\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make(\"FrozenLake-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the function to implement policy iteration. This takes in the following parameters:\n",
        "\n",
        "- Policy\n",
        "- Environment\n",
        "- Discount Factor\n",
        "- Theta\n",
        "- Max Iterations"
      ],
      "metadata": {
        "id": "3qUpmArMGT_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F56A-CuP9PoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d1cc8e-02cf-4b9f-b9dd-50fa9e303213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# policy iteration\n",
        "def policy_iteration(policy, env, discount_factor, theta, max_iterations):\n",
        "\n",
        "    num_s = env.observation_space.n\n",
        "    num_act = env.action_space.n\n",
        "    val_func = np.zeros(num_s)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        while True:\n",
        "            d = 0\n",
        "            \n",
        "            for s in range(num_s):\n",
        "                old_val = val_func[s]\n",
        "                act_p = policy[s]\n",
        "                act_val = np.zeros(num_act)\n",
        "                \n",
        "                for a in range(num_act):\n",
        "                    for p, next, reward, done in env.P[s][a]:\n",
        "                        act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "                \n",
        "                val_func[s] = np.sum(act_p * act_val)\n",
        "                d = max(d, np.abs(old_val - val_func[s]))\n",
        "            \n",
        "            if d < theta:\n",
        "                break\n",
        "        \n",
        "        policy_stable = True\n",
        "\n",
        "        for s in range(num_s):\n",
        "            old_act = np.argmax(policy[s])\n",
        "            act_val = np.zeros(num_act)\n",
        "            \n",
        "            for a in range(num_act):\n",
        "                for p, next, reward, done in env.P[s][a]:\n",
        "                    act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "            \n",
        "            best_act = np.argmax(act_val)\n",
        "            \n",
        "            if old_act != best_act:\n",
        "                policy_stable = False\n",
        "            \n",
        "            policy[s] = np.eye(num_act)[best_act]\n",
        "        \n",
        "        if policy_stable:\n",
        "            break\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the function to implement value iteration. This takes in the following parameters:\n",
        "\n",
        "- Environment\n",
        "- Discount Factor\n",
        "- Theta\n",
        "- Max Iterations"
      ],
      "metadata": {
        "id": "RDHuUBZ3HLZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value iteration\n",
        "def value_iteration(env, discount_factor, theta, max_iterations):\n",
        "\n",
        "    num_s = env.observation_space.n\n",
        "    num_act = env.action_space.n\n",
        "    val_func = np.zeros(num_s)\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        d = 0\n",
        "\n",
        "        for s in range(num_s):\n",
        "            old_val = val_func[s]\n",
        "            act_val = np.zeros(num_act)\n",
        "\n",
        "            for a in range(num_act):\n",
        "                for p, next, reward, done in env.P[s][a]:\n",
        "                    act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "\n",
        "            val_func[s] = np.max(act_val)\n",
        "            d = max(d, np.abs(old_val - val_func[s]))\n",
        "\n",
        "        if d < theta:\n",
        "            break\n",
        "    \n",
        "    policy = np.zeros((num_s, num_act))\n",
        "    for s in range(num_s):\n",
        "        act_val = np.zeros(num_act)\n",
        "\n",
        "        for a in range(num_act):\n",
        "            for p, next, reward, done in env.P[s][a]:\n",
        "                act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "\n",
        "        best_act = np.argmax(act_val)\n",
        "        policy[s][best_act] = 1.0\n",
        "    \n",
        "    return policy"
      ],
      "metadata": {
        "id": "Ys5NRBy7FX5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a functions to implement the policy iteration and value iteration functions for the given number of episodes."
      ],
      "metadata": {
        "id": "pz09z6fRHPYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the two techniques\n",
        "def episodes(policy, env, episodes):\n",
        "    tot_reward = 0\n",
        "    wins = 0\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        flag = False\n",
        "        \n",
        "        while not flag:\n",
        "            act = np.random.choice(env.action_space.n, p=policy[state])\n",
        "            next, reward, flag, _ = env.step(act)\n",
        "            ep_reward += reward\n",
        "            state = next\n",
        "        tot_reward += ep_reward\n",
        "        \n",
        "        if ep_reward == 1:\n",
        "            wins += 1\n",
        "    \n",
        "    avg_reward = tot_reward/episodes\n",
        "\n",
        "    return wins, avg_reward"
      ],
      "metadata": {
        "id": "5L-Nel02Fc9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the parameters which are passed to the functions."
      ],
      "metadata": {
        "id": "VdKCrDfLHY-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "discount_factor = 0.5\n",
        "theta = 1e-8\n",
        "max_iterations = 1000\n",
        "n_episode = 1000"
      ],
      "metadata": {
        "id": "-WbqBC3KFiLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the policy iteration function, we first pass an initial policy and then obtain the optimal policy using the policy iteration function."
      ],
      "metadata": {
        "id": "ORyC6zS3Hjtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal policy from policy iteration\n",
        "policy = np.ones([n_states, n_actions]) / n_actions\n",
        "opt_policy_pi = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
        "wins_pi, avg_reward_pi = episodes(opt_policy_pi, env, n_episode)"
      ],
      "metadata": {
        "id": "1fyylqwIFkc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the value iteration function, we first pass an initial policy and then obtain the optimal policy using the value iteration function."
      ],
      "metadata": {
        "id": "67HrtaVUHxNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal policy from value iteration\n",
        "opt_policy_vi = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "wins_vi, avg_reward_vi = episodes(opt_policy_vi, env, n_episode)"
      ],
      "metadata": {
        "id": "D8FrsHJaFmPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the parameters."
      ],
      "metadata": {
        "id": "_qYvMa3iH-5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the parameters\n",
        "print(\"Policy Iteration:\", f\"Number of wins = {wins_pi}\", f\"Average Return = {avg_reward_pi}\",\"\", sep = '\\n')\n",
        "print(\"Value Iteration:\", f\"Number of wins = {wins_vi}\", f\"Average Return = {avg_reward_vi}\",\"\", sep = '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYqSV8wUFntI",
        "outputId": "88f797dd-5e0c-4a6f-bf92-d73a7965d423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration:\n",
            "Number of wins = 458\n",
            "Average Return = 0.458\n",
            "\n",
            "Value Iteration:\n",
            "Number of wins = 455\n",
            "Average Return = 0.455\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that for policy iteration, it has a higher number of wins and hence higher average reward. This implies that policy iteration may have started to converge to the optimal policy faster than value iteration. This is applicable only when the discount factor is 0.5 and the results may change for different values of discount factor. However, the difference is not significant.\n",
        "\n",
        "Thus, from the results from this notebook, (at a discount factor of 0.5, max iterations of 1000) policy iteration is the better choice to solve the FrozenLake-v1 enviorment. "
      ],
      "metadata": {
        "id": "5MLGP83QFtVh"
      }
    }
  ]
}