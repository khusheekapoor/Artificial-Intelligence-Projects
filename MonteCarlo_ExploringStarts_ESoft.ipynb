{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 9 - AI Lab\n",
        "\n",
        "Author: Khushee Kapoor\n",
        "\n",
        "Registration Number: 200968052"
      ],
      "metadata": {
        "id": "qG-uRpwjrKTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we import the gym and numpy libraries."
      ],
      "metadata": {
        "id": "-ZHNAKVxrQFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_wezo8fEJq"
      },
      "outputs": [],
      "source": [
        "# importing the libraries\n",
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create the CliffWalking Environment by importing it from the gym."
      ],
      "metadata": {
        "id": "-Sj0TUtarT16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jQo3LKFfINu",
        "outputId": "67600df8-9375-457e-a4c7-b3e6849bb723"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# creating the environment\n",
        "env = gym.make('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code defines a function monte_carlo_es which implements the Monte Carlo ES (Exploring Starts) algorithm to learn the optimal policy for the Cliff Walking environment.\n",
        "\n",
        "The input to the function is the OpenAI Gym environment env and the number of episodes n_episodes for which the algorithm should run.\n",
        "\n",
        "Inside the function, we initialize the state-action value function Q and the visit count N to zero. We also set the discount factor gamma to 1.0, which implies that we are considering undiscounted episodes.\n",
        "\n",
        "In each episode, we reset the environment to the starting state and generate an episode using exploring starts. We choose a random action at the start of each episode to ensure that we explore all possible state-action pairs. We collect the sequence of (state, action, reward) tuples obtained during the episode in a list called episode.\n",
        "\n",
        "After generating the episode, we update the state-action values using the Monte Carlo method. We calculate the returns for each time step of the episode by summing the rewards obtained from that time step till the end of the episode. We then update the Q values for each state-action pair encountered in the episode by incrementally averaging the returns. We also update the visit count N for each state-action pair.\n",
        "\n",
        "Finally, we derive the optimal policy from the Q values by selecting the action that maximizes the Q value for each state. We return the optimal policy, the state-action value function Q, and the list of total steps taken in each episode.\n",
        "\n",
        "Overall, the monte_carlo_es function runs the Monte Carlo ES algorithm for the specified number of episodes and returns the learned optimal policy and the total steps taken in each episode."
      ],
      "metadata": {
        "id": "6z-WjIVutoUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhZ_ep0JfJzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135370ce-5bf9-41ff-d9b5-e9fb23dfc61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Monte Carlo ES (Exploring Starts)\n",
        "def monte_carlo_es(env, n_episodes=500):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    gamma = 1.0\n",
        "    total_steps = []\n",
        "    \n",
        "    for i in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        episode = []\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        # generate an episode using exploring starts\n",
        "        while not done:\n",
        "            action = np.random.choice(env.action_space.n)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        total_steps.append(steps)\n",
        "        \n",
        "        # update Q values using the episode\n",
        "        returns = 0\n",
        "        for j in range(len(episode)-1, -1, -1):\n",
        "            state, action, reward = episode[j]\n",
        "            returns = gamma*returns + reward\n",
        "            N[state][action] += 1\n",
        "            Q[state][action] += (returns - Q[state][action])/N[state][action]\n",
        "    \n",
        "    # derive optimal policy from Q values\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    \n",
        "    return policy, Q, total_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function on_policy_mc_control which implements the on-policy first-visit Monte Carlo control algorithm with Ɛ-soft policies to learn the optimal policy for the Cliff Walking environment.\n",
        "\n",
        "The input to the function is the OpenAI Gym environment env, the number of episodes n_episodes for which the algorithm should run, and the Ɛ parameter of the Ɛ-soft policy epsilon.\n",
        "\n",
        "Inside the function, we initialize the state-action value function Q and the visit count N to zero. We also set the discount factor gamma to 1.0, which implies that we are considering undiscounted episodes.\n",
        "\n",
        "In each episode, we reset the environment to the starting state and generate an episode using an Ɛ-soft policy. At each time step, we choose a random action with probability Ɛ or the greedy action (i.e., the action that maximizes the Q value) with probability 1 - Ɛ. We update the Q values using the incrementally averaged returns obtained from the episode. We also update the visit count N for each state-action pair encountered in the episode.\n",
        "\n",
        "Finally, we derive the optimal policy from the Q values by selecting the action that maximizes the Q value for each state. We return the optimal policy, the state-action value function Q, and the list of total steps taken in each episode.\n",
        "\n",
        "Overall, the on_policy_mc_control function runs the on-policy first-visit Monte Carlo control algorithm with Ɛ-soft policies for the specified number of episodes and returns the learned optimal policy and the total steps taken in each episode."
      ],
      "metadata": {
        "id": "c_aqr0BStrGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q56B6E7FfOjX"
      },
      "outputs": [],
      "source": [
        "# On-policy first-visit MC control (for Ɛ-soft policies), for Ɛ = 0.1\n",
        "def on_policy_mc_control(env, n_episodes=500, epsilon=0.1):\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    gamma = 1.0\n",
        "    total_steps = []\n",
        "    \n",
        "    for i in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        \n",
        "        # generate an episode using Ɛ-soft policy\n",
        "        while not done:\n",
        "            if np.random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            N[state][action] += 1\n",
        "            Q[state][action] += (reward + gamma*np.max(Q[next_state]) - Q[state][action])/N[state][action]\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        total_steps.append(steps)\n",
        "    \n",
        "    # derive optimal policy from Q values\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "    \n",
        "    return policy, Q, total_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code snippet applies the monte_carlo_es and on_policy_mc_control functions to the Cliff Walking environment env to learn the optimal policy using two different algorithms: Monte Carlo ES and On-policy first-visit MC control with Ɛ-soft policies.\n",
        "\n",
        "The output of the monte_carlo_es function is three variables: monte_carlo_es_policy, monte_carlo_es_q, and total_steps_es.\n",
        "\n",
        "- monte_carlo_es_policy is the learned optimal policy as an array of shape (48,), where the i-th element is the action that maximizes the Q value for state i.\n",
        "\n",
        "- monte_carlo_es_q is the state-action value function Q as an array of shape (48,4), where the i-th row corresponds to the Q values for the i-th state and the j-th column corresponds to the Q value for taking action j in state i.\n",
        "\n",
        "- total_steps_es is a list of length n_episodes containing the total number of steps taken in each episode during the Monte Carlo ES algorithm.\n",
        "\n",
        "Similarly, the output of the on_policy_mc_control function is three variables: on_policy_mc_control_policy, on_policy_mc_control_q, and total_steps_control.\n",
        "\n",
        "- on_policy_mc_control_policy is the learned optimal policy as an array of shape (48,), where the i-th element is the action that maximizes the Q value for state i.\n",
        "\n",
        "- on_policy_mc_control_q is the state-action value function Q as an array of shape (48,4), where the i-th row corresponds to the Q values for the i-th state and the j-th column corresponds to the Q value for taking action j in state i.\n",
        "\n",
        "- total_steps_control is a list of length n_episodes containing the total number of steps taken in each episode during the On-policy first-visit MC control algorithm.\n",
        "\n",
        "Overall, this code snippet applies two different algorithms to learn the optimal policy for the Cliff Walking environment and stores the learned policy, Q function, and total steps taken in each episode for both algorithms."
      ],
      "metadata": {
        "id": "UZfbYBk6uKoI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJOLrSfqfVOY",
        "outputId": "b8434517-e0c1-4484-a5c0-e8b7f769243a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# run Monte Carlo ES and On-policy first-visit MC control\n",
        "monte_carlo_es_policy, monte_carlo_es_q, total_steps_es = monte_carlo_es(env)\n",
        "on_policy_mc_control_policy, on_policy_mc_control_q, total_steps_control = on_policy_mc_control(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compare the total number of steps taken to reach the optimal policy using both the techniques by summing over the total number of steps taken in each episode."
      ],
      "metadata": {
        "id": "Qd-EsBQauWl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of steps taken to reach optimal policy\n",
        "print(str.format('Total Number of Steps taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)))\n",
        "print(str.format('Total Number of Steps taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIqvUHHNnFH_",
        "outputId": "22835444-5344-4337-e6c5-e769b49efc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Steps taken to reach Optimal Policy using Monte Carlo ES: 3084474\n",
            "Total Number of Steps taken to reach Optimal Policy using On-Policy First-Visit MC Control: 17440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the number of steps taken in the On-Policy First-Visit MC Control technique is significantly less than the Monte Carlo ES technique.\n",
        "\n",
        "Similarly, we compare the average number of steps taken to reach the optimal policy using both the techniques by summing over the total number of steps taken in each episode and dividing it by the total number of episodes."
      ],
      "metadata": {
        "id": "6sJbzfzpufI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# average number of steps per episode taken to reach optimal policy\n",
        "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using Monte Carlo ES: {}', sum(total_steps_es)/len(total_steps_es)))\n",
        "print(str.format('Average Number of Steps per Episode taken to reach Optimal Policy using On-Policy First-Visit MC Control: {}', sum(total_steps_control)/len(total_steps_control)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2Y6RMNypqaL",
        "outputId": "0a89bb90-657f-4736-f6b7-32dc7ed8c06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Number of Steps per Episode taken to reach Optimal Policy using Monte Carlo ES: 6168.948\n",
            "Average Number of Steps per Episode taken to reach Optimal Policy using On-Policy First-Visit MC Control: 34.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the average number of steps taken per episode in the On-Policy First-Visit MC Control technique is significantly less than the Monte Carlo ES technique.\n"
      ],
      "metadata": {
        "id": "vpzz-JjruyW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above comparison imply that the On-Policy First-Visit MC Control technique converges to the Optimal Policy faster than the Monte Carlo ES technique, and hence, has a better performance."
      ],
      "metadata": {
        "id": "7YzCC5Q_u2g-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}